---
title: "Reading Data from APIs"
output: github_document
    theme: flatly
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(httr)
library(jsonlite)
library(ggplot2)
library(tidyjson)
library(plotly)

```

## Requirements

The packages used in this vignette are 

[tidyverse](https://www.tidyverse.org/)
[httr](https://httr2.r-lib.org/)
[jsonlite](https://cran.r-project.org/package=jsonlite)

## API

I wanted to query the National Parks service API which provides information about US National Parks. In order to interact with this API you will need an api key. You can register for a free API key [here](https://www.nps.gov/subjects/developer/get-started.htm). This API allows 1,000 requests per hour! That's pretty generous as far as APIs go. You can also read more information about it in the [API Guide](https://www.nps.gov/subjects/developer/guides.htm). In order to keep my own API key a secret I have aliased it in the remaining code as `my_api_key`, but you can create a variable called `my_api_key` which will be the default of the functions in this vignette, or you can simply enter your api key into the functions.  


```{r, echo = FALSE}

#nothing to see here
my_api_key <- 'iGcTVkjLRg4ruBc2hld67MDP0XtzaeEygeY0eUlf'

```


## Functions

There are several functions I created in order to interact with the API. 

### Helper Functions

#### `get_state_codes`

The first is a helper function `get_state_codes`. For some of the API endpoints the National Parks Service allows the user to enter the two character state abbreviation. In our final API function we want to be a bit more flexible and allow the user to be able to enter the name of a state, the two digit state code, or two leave the state parameter blank (the default will be null) to return all states. The `get_state_codes` function takes one parameter which is either the name of a state, the state abbreviation, and either returns a corresponding state abbrevation or ensures that the state abbreviation entered is valid. It uses the built in R datasets [state.abb](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/state) and [state.name](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/state).


```{r}

get_state_codes <- function(state) {
    
    #convert the state parameter to lower case
    state <- tolower(state)
    #convert built in state abbreviations and state names to lower case
    state_abbs <- tolower(state.abb)
    state_names <- tolower(state.name)
    
    #if the state entered by the user is a state name:
    if(state %in% state_names) {
        
         #get the index of the state.name and return the abbreviation at that index
        state <- state_abbs[which(state_names == state)]
        
    }
    
    #if the state is not an abbreviation (or has been converted to one by the previous if statement)
    if(state %in% state_abbs) {
        
        #return the abbreviation
        return(state)
        
    #if the state is not an abbreviation
    }else{
        
        #output a helpful error message
        stop("Please enter a valid state name or abbreviation or 'all' to query all states")
        
    }
    
} 


```

#### `get_max_results`

The National Parks APIs allow the user to set a limit of how many results to return, and if the user does not specify a default limit is set (usually 50 for most endpoints). This is helpful if you don't want to return too many results, but can be problematic if you don't know how many results there actually are. Thankfully when the data is returned the total number of possible results is also returned in a field called "total" as well as the limit set in a field called "limit". Using these fields I created a function to make sure we don't inadvertently return too few results. 

```{r}
get_max_results <- function(url) {
    
    
    #call the api and get results
    api_call <- GET(url)
    
    #parse the data from JSON
    parsed_data <- fromJSON(rawToChar(api_call$content), simplifyDataFrame = TRUE)
    
    #if the number of results possible is greater than the number of results returned, call the api again with the max possible results
    if(as.numeric(parsed_data$limit) < as.numeric(parsed_data$total)) {
        
        url <- paste0(url, '&limit=', parsed_data$total)
        
        api_call <- GET(url)
        
        #parsed_data <- fromJSON(rawToChar(api_call$content))
    
    }
    
    #return the parsed_data
    return(api_call)
    
}

```

#### `get_park_codes`

Some NPS API endpoints do not allow the user to enter a state and instead require the park code. These definitely are not values that I would expect many people to know off of the top of their head. To help the user out I created another helper function called `get_park_codes`. This function has a state parameter which can be left blank (default null) to select all park codes, or using the `get_state_code` helper function the user can enter a state abbreviation or name. The function calls the parks endpoint and returns a character vector of all of the park codes associated with that state. 


```{r}

get_park_codes <- function(state = NULL, api_key = my_api_key) {
    
     #start with a base url of the api call
    url <-  'https://developer.nps.gov/api/v1/parks?'
    
    #if the state parameter is not null
    if(!is.null(state)) {
        
        #get the state code
        state_code <- get_state_codes(state)
        
        #add the state code parameter to the url
        url <- paste0(url,'&stateCode=', state_code)
    
    }
    
    #add the api key to the url
    url <- paste0(url,'&api_key=', api_key)
   
    #get the max number of results available from the api
    res <- get_max_results(url)
    
    #parse the content
    content <- fromJSON(rawToChar(res$content))
    
    #output a dataframe with the distinct park names and codes
    out <- content$data %>% 
        select(parkCode, fullName) %>%
        distinct()
    
    return(out)
    
}

```

Once our data has been returned from the API and parsed we will need a little extra help flattening it into a neat tibble or a list of tibbles. For example when calling the parks API endpoint the data returned contains list columns for certain variables even after using `fromJSON` in the [jsonlite](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html) package. This function takes a tibble or data.frame that contains list columns and a character vector for the column name. It returns a tibble with the park code and the values of the list column as a tibble. For this we use the `unnest` function from the [purrr](https://purrr.tidyverse.org/) package. 

```{r}

df <- out[[2]] %>%
    
new_df <- handle_list_columns('amenities.toilets', df) 
    

    mutate_at(amenities.toilets, ~map(., select, -matches('id')))
    
    select(id, amenities.toilets) %>%
    filter(lengths(amenities.toilets) != 0) %>%

column_names <- out[[1]] 

test <- column_names %>%
    map(handle_list_columns, df)

handle_list_columns <- function(list_column, .data) {
    
    #ensym the list_column parameter so that it may be passed in the functions
    column_name <- ensym(list_column)
    
    out <- .data %>%
        #select the id and the list column name
        select(id,!!column_name) %>%
        #remove empty columns
        filter(lengths(!!column_name) != 0) %>%
        #unnest the list column 
        unnest(!!column_name, names_sep = '')
   
   #return the data
   return(out)
}


```


```{r}

rectangle_data <- function(.data, output_tibble = FALSE, datasets = NULL) {
    
    #flatten the data and remove columns we are not interested in
    dat <- flatten(.data) %>%
        select(-matches('passportStampImages|images|multimedia|contacts|addresses|operatingHours')) 
        
    #create a data frame with all of the columns that are character vectors (i.e are already flat)
    flat_data <- dat %>%
        select_if(is.character)
    
    #select columns that are not character vector columns along with the id (for joining later or associating with other datasets)
    #these columns will need to be flattened separately due to ragged hierarchy
    list_columns <- dat %>%
        select(id, where(negate(is.character)))
    
    #get a character vector of the names of the columns that need to be unnested further
    list_cols <- list_columns %>%
        select(-id) %>%
        names()
    
    #handle the remaing list columns using our handle_list_columns function
    out <- as.list(list_cols) %>%
        set_names(list_cols) %>%
        map(handle_list_columns, list_columns) %>%
        #remove empty dataframes from the resulting list of dataframes
        discard(~nrow(.x) == 0) %>%
        #prepend the flattened data to the begining of the list of dataframes
        append(list('main_data' = flat_data),  after = 0)
    
    #if the user entered the datasets parameter
    if(!is.null(datasets)) {
        
        #if all of the dataset names the user entered are valid
        if(all(datasets %in% names(out))) {
        
            #if the user did not select to include all of the main data
            if(!'main_data' %in% datasets) {
            
              #get the ids parkCodes and name/fullName from the main data
              all_ids <- out$main_data %>%
                 select(id, parkCode, matches('fullName|name'))
            
                #subset the data to just the datasets the user selected
                 out <- out[datasets] %>%
                #prepend all of the ids to the list of dataframes selected
                append(list('all_ids' = all_ids), after = 0)
            
         #subset the data to just the datasets the user selected
            } else  {
                out <- out[datasets] 
                }
        } else {
            #throw an error and output a helpful message
            stop("Please enter valid dataset(s)")
        }
        
        
       
    }
    
     #if the user selects the output to be a tibble left join the datasets together on the parkcode
    if(output_tibble) {
        
        out <- out %>%
            reduce(left_join, by = 'id')
    }
    
    #return data
    return(out)
    
}

```


### API Functions

#### `get_park_info`

With the helper functions set, I'm ready to interact with the API to bring back data. The first endpoint I want to interact with is the parks endpoint. This endpoint provides a lot of information about each park as we will see. To bring back data from this endpoint I created a function `get_park_info`. The endpoint allows the user to enter a state or a park code, so we will use our `get_state_codes` helper function, but we won't need `get_park_codes` this time. We will also need our `get_max_results` and `rectangle_data` functions.


I also wanted to explore the campgrounds available at the parks. The NPS API has a campground endpoint which provides information about different campsites, how many spots are available, hours of operation, types of campsites, cost, etc. To gather this data I created a `get_campground_info` function.

```{r}

get_park_info <- function(state = NULL, park_code = NULL, api_key = my_api_key, endpoint = 'campgrounds', ...) { 
    
    #start with a base url of the api call and add the endpoint
    url <-  paste0('https://developer.nps.gov/api/v1/', endpoint, '?')
    
    #if the user enters a state name or code
    if(!is.null(state)) {
        
        #get the state code
        state_code <- get_state_codes(state)
        
        #add the state code parameter to the url
        url <- paste0(url,'&stateCode=', state_code)
    
    }
    
    #if user enters a park code
    if(!is.null(park_code)) {
        
        #add the park_code to the url
       url <- paste0(url,'&parkCode=', park_code) 
    }
    
    #add the api key to the url
    url <- paste0(url,'&api_key=', my_api_key)
   
    #get the maximum restuls from the api
    res <- get_max_results(url)
    
    #extract the content from the JSON object
    content <- fromJSON(rawToChar(res$content))
    
    #extract the data
    dat <- content$data
    
    #get the data into a list of dataframes or a single tibble based on user selection
    out <- rectangle_data(dat, ...)
    
    #rectangle the datasets specified by the user
    #out <- datasets %>%
        #name datasets list with it's own character vectors (e.g, list('activities' = 'activities', 'topics' = 'topics'))
     #   set_names(.) %>%
        #map over the rectangle data function to apply it to each dataset selected this will return a list of tibbles
#        map(~rectangle_data(.data = res$data, col_name = .)) %>%
        #prepend the remaining park information that does not contain list columns to the list
     #   append(list('data' = res$data %>% discard(is.list) %>% select(-id) %>% tibble()), after = 0)
    
    #if the user selects the output to be a tibble left join the datasets together on the parkcode
  #  if(output_tibble) {
        
   #     out <- out %>%
   #         reduce(left_join, by = c('parkCode', 'name'))
   # }
    
    #return data
    return(out)
    
}

```

The next two API endpoints I wanted to explore are related to amenities of the parks. There is "amenities/parksplaces" which provides information about the amenities of places within each park, and "amenities/parksvisitorcenters" which provides information about the amenities of the park visitor centers. This endpoint allows search terms such as "wheelchair access" or "accessibility"

```{r}

park_data <- get_park_info( endpoint = 'campgrounds', output_tibble = TRUE, datasets = 'fees')


all(c('topics','amenities') %in% names(park_data))

test <- park_data[datas]

call <- GET(paste0('https://developer.nps.gov/api/v1/campgrounds?', '&api_key=', my_api_key))
           

content <- fromJSON(rawToChar(call$content))

dat <- content$data





    
    
    
    
    mutate(activities = map(activities, ~select(.,-id)) %>% 
    

test <- as.list(list_cols) %>%
    set_names(list_cols) %>%
    map(handle_list_columns, lists) %>%
    discard(~nrow(.x) == 0)

test <- handle_list_columns(fees, lists)


fees <- lists %>%
    select(id,fees) %>%
    filter(lengths(fees) != 0) %>%
    unnest(fees)

operating_hours <- lists %>%
    select(id,operatingHours) %>%
    filter(lengths(operatingHours) != 0) %>%
    unnest(operatingHours)
    
exceptions <- operating_hours %>%
    select(id, exceptions) %>%
    filter(lengths(exceptions) != 0) %>%
    unnest(exceptions)


amenties <- lists %>%
    select(id, amenities.potableWater) %>%
    filter(lengths(amenities.potableWater) != 0) %>%
    unnest(amenities.potableWater)

    
    map(enframe) %>%
    map(unnest) %>%
    map(class)
    enframe() %>%
    unnest() %>%
    
    select_if(is.character)

not_flat_data <- dat %>%
    flatten() %>%
    select_if(negate(is.character)) %>%
    map(enframe) %>%
    map(unnest) %>%
    map(select, -name) %>%
    map_at('operatingHours', flatten)

test <- fromJSON(rawToChar(park_data$content))$data

test_3 <-

test_2 <- fromJSON(rawToChar(park_data$content), flatten = FALSE)$data %>%
    select_if(is.data.frame)

setdiff(names(test_2), names(test))

identical(test$data,test_2$data)

test_spread <- test %>%  
    enter_object(data) %>% 
    gather_array() %>% 
    spread_all() %>%
    select(id,url,fullName,parkCode,description,latitude,latLong,states,directionsInfo,weatherInfo,name,designation)

data <- test_spread %>%
    select()
 
test_JSON <- test_spread$..JSON[[1]]$data

campground_data <- get_park_info( output_tibble = FALSE)

park_data <- get_park_info(endpoint = 'parks', output_tibble = FALSE) 

park_names <- park_data$data %>%
    select(parkCode, fullName)

campsites <- campground_data$data %>%
    select(name, parkCode, numberOfSitesReservable, numberOfSitesFirstComeFirstServe) %>%
    mutate_at(vars(numberOfSitesReservable, numberOfSitesFirstComeFirstServe), as.numeric) %>%
    group_by(name, parkCode) %>%
    pivot_longer(cols = matches('Sites'), names_to = 'site_type', values_to = 'campsites') %>%
    mutate(site_type = str_remove_all(site_type, 'numberOfSites')) %>%
    inner_join(park_names, by = 'parkCode')
    
    

x <- as.factor(c('a','b','b','c','f'))

x[4] <- 'd'

#which park has more walkup sites
ggplot(data = campsites, aes(x = fullName, y = campsites, fill = site_type)) +
    geom_bar(position = 'stack', stat = 'identity') +
    scale_x_discrete(guide = guide_axis(angle = 30)) +
    scale_fill_discrete(name = "") +
    theme_minimal() +
    labs(x = NULL, title = 'Number of Campsites by Reservability', subtitle = 'National Parks of North Carolina') 
    
    
    
    facet_wrap(~fullName, scales = 'free') +


campgrounds_by_park <- park_data$data %>%
        select(parkCode, fullName) %>%
        left_join(campground_data %>% 
                      select(parkCode, name) %>%
                      , by = 'parkCode')

        
activities_by_park <- park_data$data %>%
        select(parkCode, fullName) %>%
        left_join(park_data$activities , by = 'parkCode') %>%
        select(fullName, activity = activities_activities_name) %>%
        distinct() %>%
        mutate(dummy = 1) %>%
        group_by(activity) %>%
        pivot_wider(names_from = fullName, values_from = dummy, values_fill = 0)
    

    
```


lets test out our function: 

```{r}

test <- get_park_info(state = 'North Carolina', datasets = c('activities', 'entranceFees'), output_tibble = FALSE)

```

### Scatter Plot

```{r}

#avg fee vs total sites

park_names <- get_park_codes()

campgrounds <- get_park_info(endpoint = 'campgrounds',
                             datasets = c('main_data'), 
                             output_tibble = TRUE) %>%
    select(id,parkCode,name) %>%
    inner_join(park_names, by = 'parkCode')




campground_fees <- get_park_info(endpoint = 'campgrounds', 
                                          datasets = c('fees'),
                                          output_tibble = TRUE)


campground_sites <- get_park_info(endpoint = 'campgrounds', 
                                  datasets = c('main_data'), 
                                  output_tibble = TRUE)

sites_per_campground <- campground_sites %>%
    mutate(reservable = as.numeric(numberOfSitesReservable), 
           walkup = as.numeric(numberOfSitesFirstComeFirstServe),
           total_sites = as.numeric(campsites.totalSites)) %>%
    select(id, parkCode, name, reservable, walkup, total_sites)

sites_and_fees <- sites_per_campground %>%
    inner_join(campground_fees, by = c('id', 'parkCode', 'name'))
    


ggplotly(sites_and_fees_plot)

```



### Box Plot

```{r}

campground_fee_data <- get_park_info(datasets = 'fees', endpoint = 'campgrounds')

fees <- campground_data$fees %>%
    select(parkCode, cost = fees_fees_cost) %>%
    inner_join(park_data$data %>% select(fullName, parkCode)) %>%
    mutate(cost = as.numeric(cost)) 

ggplot(data = fees, aes(x = cost)) +
    geom_histogram()

ggplot(data = fees, aes(y = cost, fill = fullName)) +
    geom_boxplot()



```


#### Contingency Tables

Which parks have the most amenities

```{r}

campground_data <- get_park_info(endpoint = 'campgrounds', datasets = c('accessibility', 'fees'), output_tibble = TRUE)


campground_data %>%
    select(matches('wheelchair')) %>%
    rename('wheelchairAccess') %>%
    distinct()

amenities <- get_max_results(paste0("https://developer.nps.gov/api/v1/amenities/parksplaces?",'&api_key=', my_api_key))
amenities_visitor_center <- get_max_results(paste0("https://developer.nps.gov/api/v1/amenities/parksvisitorcenters?",'&api_key=', my_api_key))

parsed_amenities <- amenities_visitor_center$data %>%
    map(pluck, 'parks') %>%
    map(as.data.frame) %>%
    set_names(parsed_amenity_names) %>%
    compact() %>%
    map(unnest, visitorcenters, names_sep = '_') %>%
    map(select, states, parkCode, fullName, visitorcenters_name) %>%
    enframe() %>%
    unnest(value) %>% 
    separate_rows(states) %>%
    group_by(parkCode, fullName, name) %>%
    summarise(n_visitorcenters = n_distinct(visitorcenters_name), n_states = n_distinct(states)) %>%
    ungroup() %>%
    group_by(parkCode, fullName, n_states) %>%
    pivot_wider(names_from = name, values_from = n_visitorcenters, values_fn = sum, values_fill = 0) %>%
    ungroup()


test_rooms <- parsed_amenities %>%
    filter(str_detect(name, 'Restroom')) %>%
    filter(str_detect(fullName, 'Blue Ridge'))
    
restrooms <- parsed_amenities %>%
    select(parkCode, fullName, matches('Restroom')) %>%
    filter(Restroom > 0) %>%
    mutate(accessibility_ratio = `Restroom - Accessible`/Restroom)

parsed_amenities$n_states

parsed_amenities %>%
    group_by(parkCode, fullName)
    
    
    
    map(separate_rows, states) %>%
    map(distinct) %>%
    map(group_by, parkCode, fullName) %>%
    map(summarise, n_places = n_distinct(places_title), n_states = n_distinct(states)) 

parkCode, fullName, places_title, amenity_name, states


test <- parsed_amenities$`Restroom` %>%
    select(states, parkCode, fullName, places_title) %>%
    separate_rows(states) %>%
    distinct() %>%
    group_by(parkCode, fullName) %>%
    summarise(n_places = n_distinct(places_title), n_states = n_distinct(states))

test <- parsed_amenities$`Restroom` %>%
    select(states, parkCode, fullName, places_title) %>%
    separate_rows(states) %>%
    distinct() %>%
    group_by(parkCode, fullName) %>%
    summarise(n_places = n_distinct(places_title), n_states = n_distinct(states))

```


#### Numerical Summaries

#### Plots



```{r}



what <- test %>%
    discard(is.list)

test <- get_park_info(state = 'MI')

#get the names of each of the list columns
list_col_names <- test %>%
    map(class) %>%
    keep(has_element, 'list') %>%
    names()

col_name <- 'operatingHours'

this <- test %>%
    select(fees)


this <- test %>%
    
    
this <- test %>%
    unnest(cols = these)
    
    

new <- test %>%
    select(parkCode, all_of(col_name)) %>%
    unnest(col_name) %>%
    select(-matches('id|exceptions')) %>%
    rename_at(vars(-parkCode), ~paste0(col_name, '_', .)) %>%
    flatten()



    unnest_longer('activities') %>%
    unnest_longer('topics') %>%
    unnest_longer('entranceFees') %>%
    unnest_longer('entrancePasses') %>%
    unnest_longer('operatingHours') %>%
    unnest_longer('operatingHours')

new <- test %>%
    select(parkCode, fees)

```

Now let's test out our function:

```{r}

test <- get_park_info(state = 'michigan')

test <- get_parks(state = 'nc')

as.numeric(test$limit) < as.numeric(test$total)

test <- GET(api_url) 

test_again <- fromJSON(rawToChar(test$content))


visitation <- GET('https://irma.nps.gov/Stats/SSRSReports/National%20Reports/Query%20Builder%20for%20Public%20Use%20Statistics%20(1979%20-%20Last%20Calendar%20Year)')

visits <-rawToChar(visitation$content)

javascript:void(0)

```




```{r scratchpad, echo = FALSE, eval = FALSE}
state.name
state.abb

campgrounds <- GET("https://developer.nps.gov/api/v1/campgrounds?stateCode=NC&api_key=iGcTVkjLRg4ruBc2hld67MDP0XtzaeEygeY0eUlf")

alerts <- GET("https://developer.nps.gov/api/v1/alerts?api_key=iGcTVkjLRg4ruBc2hld67MDP0XtzaeEygeY0eUlf")

test <- fromJSON(rawToChar(alerts$content))

parsed_campgrounds <- fromJSON(rawToChar(campgrounds$content)) %>%
    unnest(amenities, names_sep = '_') %>%
    unnest(fees, names_sep = '_') %>%
    unnest(passportStampImages, names_sep = '_')

amenities <- parsed_campgrounds$amenities
    
?unnest    

activities <- GET("https://developer.nps.gov/api/v1/activities/parks?limit=100&api_key=iGcTVkjLRg4ruBc2hld67MDP0XtzaeEygeY0eUlf")

activities$content

parsed_activities <- fromJSON(rawToChar(activities$content))

combined_activities <- parsed_activities$data %>%
    rename('activity' = name) %>%
    unnest(parks)

activities_with_campgrounds <- parsed_campgrounds %>%
    inner_join(combined_activities, by = park_code)



```








```{r}


visitation_data <- read_csv('Public Use Statistics (1979 - Last Calendar Year).csv')

```


## Exploratory Analysis

```{r}
#EDA Requirements (A few requirements about your EDA are below:
#∗ You should pull data from at least two separate calls to your API functions
#∗ You should create some contingency tables
#∗ You should create numerical summaries for some quantitative variables at each setting of some
#of your categorical variables
#∗ You should create at least five plots utilizing coloring, grouping, etc. All plots should have
#nice labels and titles.
#· At least one plot that you create should be a plot that we didn’t cover in class (say a
#heatmap or something like that - lots of good examples to consider here https://exts.
#ggplot2.tidyverse.org/gallery/))


```

